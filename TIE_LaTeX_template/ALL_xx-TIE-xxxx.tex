\documentclass[journal]{IEEEtranTIE}

\usepackage{graphicx}
\usepackage{cite}
\usepackage{picinpar}
\usepackage{amsmath}
\usepackage{url}
\usepackage{flushend}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{color}
\usepackage{alltt}
\usepackage[hidelinks]{hyperref}
\usepackage{enumerate}
\usepackage{siunitx}
\usepackage{breakurl}
\usepackage{epstopdf}
\usepackage{pbox}
\usepackage{booktabs}

\begin{document}

\title{Reinforcement Learning Strategies for Automated Filling Control}

\author{%
\"{O}mer Sabri Emeksiz, \emph{Student Member, IEEE},\ Engin Ma\c{s}azade, \emph{Member, IEEE},\\
and Suat Selim, \emph{Member, IEEE}%
\thanks{Manuscript prepared Month~xx,~2024; revised Month~xx,~2024.}
\thanks{\"{O}.~S. Emeksiz is with Ko\c{c} University, Department of Computer Engineering, Istanbul 34450, Turkey (e-mail: oemeksiz24@ku.edu.tr).}
\thanks{E. Ma\c{s}azade is with Marmara University, Department of Electrical and Electronics Engineering, Istanbul 34722, Turkey (e-mail: engin.masazade@marmara.edu.tr).}
\thanks{S. Selim is with BAYKON Industrial Weighing Systems, Istanbul 34775, Turkey (e-mail: sselim@baykon.com).}}

\maketitle

\begin{abstract}
Industrial filling systems normally operate two-stage actuators: a high-throughput ``fast'' mode followed by a ``slow'' top-off mode. Setting the switch point between these stages is critical for meeting strict weight tolerances without sacrificing cycle time. This paper documents an end-to-end reinforcement learning (RL) pipeline that was implemented for a real filling line. We consolidate offline production traces, build four RL agents (multi-armed bandit, Monte Carlo control, temporal-difference control, and Q-learning), and expose the trained policies through logging, visualization, and plant-integration hooks. Offline replay experiments confirm that all agents converge to high-quality switching decisions under a shared reward design, while Q-learning provides the lowest variance in final weight. The manuscript follows the IEEE Transactions on Industrial Electronics format to facilitate future submission and records the engineering choices required to reproduce and deploy the solution.
\end{abstract}

\begin{IEEEkeywords}
Reinforcement learning, Industrial automation, Filling control, Multi-armed bandit, Monte Carlo, Temporal-difference learning, Q-learning
\end{IEEEkeywords}

\markboth{IEEE Transactions on Industrial Electronics}{}

\section{Introduction}
\IEEEPARstart{R}{einforcement} learning has matured into a versatile tool for closed-loop industrial automation \cite{Sutton2018}. Many production tasks remain dominated by heuristic control because safety, interpretability, and deployment effort present significant barriers. The volumetric filling process studied in this work illustrates these challenges: an operator must switch from a fast valve position to a slow completion state while respecting a narrow safe band of \SIrange{74}{76}{g}. A late switch risks overflow and rework; an early switch increases cycle duration and throughput loss.

This study contributes an RL-based approach that augments an existing filling cell without altering the hardware. The proposed workflow mines historical filling sessions, quantizes the observable weight trajectory, and trains agents that directly output the next switching point. The following contributions are emphasized:
\begin{enumerate}[1)]
  \item An offline data processing pipeline that parses Excel-based production logs into reusable state-action trajectories with explicit switch and termination tokens.
  \item A unified RL training framework that instantiates multi-armed bandit (MAB), Monte Carlo (MC), temporal-difference (TD), and Q-learning controllers using a shared reward structure and exploration schedule.
  \item An evaluation and deployment toolkit that reproduces plots, Excel summaries, TCP/Modbus interfaces, and optional MySQL persistence for live plant integration.
\end{enumerate}
The remainder of the paper details the industrial context, algorithmic design, and engineering artifacts that enable reproducible experimentation and field trials.

\section{Related Work}
Bandit methods have long been used to balance exploration and exploitation in adaptive decision-making \cite{Lai1985}. Modern RL textbooks provide comprehensive coverage of Monte Carlo, TD, and Q-learning algorithms that underpin many industrial case studies \cite{Sutton2018}. Safety considerations for RL-controlled processes are surveyed in \cite{Garcia2015}, highlighting the importance of bounded actions and explicit penalties when interacting with physical equipment. Deployment-focused studies in robotics stress the need for explainable policies, staged validation, and high-fidelity logging \cite{Kober2013}. The present work aligns with these guidelines by constraining the action space to two valve states, embedding traceability into the logging pipeline, and providing deterministic offline replay before interfacing with live assets.

\section{Process Modeling and Dataset}
\subsection{Offline Data Acquisition}
Production data are delivered as Excel workbooks in which each column captures a single fill cycle. The upstream controller emits integer weight readings at \SI{10}{ms} intervals. Two sentinel values encode process boundaries: $-1$ marks the time step immediately after the physical switch, and $300$ denotes termination. The \texttt{DataProcessor} utility ingests the workbook, filters invalid sessions, clusters trajectories by observed switch point, and produces weight sequences amenable to replay.

The state space $\mathcal{S}$ therefore consists of quantized weights observed before ($a=1$) and after ($a=-1$) switching. The action space contains only the decision to remain in the current valve mode or to transition. Because each recorded session already includes the operator's switching decision, replaying a trajectory renders a full state-action sequence for a candidate policy.

\subsection{Reward Formulation}
The controller must minimize underfills and overfills while discouraging long cycles. The reward assigned to the terminal state is defined as
\begin{equation}
  r_T = -T + \lambda_{\text{of}} \max(0, w_T - w_{\max}) - \lambda_{\text{uf}} \max(0, w_{\min} - w_T),
  \label{eq:reward}
\end{equation}
where $T$ is the episode length (number of high-frequency samples before termination), $w_T$ is the final recorded weight, and $[w_{\min}, w_{\max}] = [74, 76]$ \si{g} defines the safe window. The coefficients $\lambda_{\text{of}}$ and $\lambda_{\text{uf}}$ penalize deviations according to production tolerances. Intermediate steps incur either a neutral reward (MC, TD, Q-learning variants) or a constant time penalty (MAB baseline) consistent with the respective training scripts.

\section{Reinforcement Learning Framework}
All agents share the same experience replay interface and exploration policy. An $\epsilon$-greedy strategy with distance-weighted exploration encourages the agents to inspect switch points surrounding the current incumbent. The weighting vector $[5, 4, 3, 2, 1]$ biases exploration towards nearby alternatives, reflecting the physically meaningful neighborhood of switch points.

\subsection{Multi-Armed Bandit Baseline}
The bandit formulation treats each discrete switch point as an arm with value $Q(\text{sp})$. After replaying a trajectory, the reward computed from \eqref{eq:reward} updates the experienced arm via
\begin{equation}
  Q_{t+1}(\text{sp}) = Q_t(\text{sp}) + \alpha \bigl( r_T - Q_t(\text{sp}) \bigr),
\end{equation}
where $\alpha = 0.4$ and $r_T$ incorporates both cycle length and safety penalties. The greedy arm becomes the default recommendation for the next episode unless exploration is triggered.

\subsection{Monte Carlo Control}
The MC agent represents state-action pairs. Each replayed trajectory accumulates the discounted return
\begin{equation}
  G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k, \quad \gamma = 0.99,
\end{equation}
which updates $Q(s_t, a_t)$ with a step size $\alpha = 0.01$. Positive updates for fast-mode actions are required before slow-mode values are trusted, guarding against sparse visitation near the switching frontier.

\subsection{Temporal-Difference Control}
The TD (SARSA) agent shares the state-action representation but performs one-step bootstrapping:
\begin{equation}
  Q_{t+1}(s_t, a_t) = Q_t(s_t, a_t) + \alpha \bigl( r_t + \gamma Q_t(s_{t+1}, a_{t+1}) - Q_t(s_t, a_t) \bigr).
\end{equation}
A relatively larger learning rate ($\alpha = 0.1$) accelerates adaptation to new samples, while $\epsilon$ is held constant at $0.5$ to promote sustained exploration in the narrow state space.

\subsection{Q-Learning}
The off-policy agent replaces the bootstrapped term with the greedy value:
\begin{equation}
  Q_{t+1}(s_t, a_t) = Q_t(s_t, a_t) + \alpha \bigl(r_t + \gamma \max_{a'} Q_t(s_{t+1}, a') - Q_t(s_t, a_t) \bigr).
\end{equation}
Initial Q-values are set to zero, and the exploration schedule decays from 0.5 to 0 with a factor of 0.995. The learned policy identifies the earliest weight at which the slow action dominates. This value becomes the recommended switch point for the next production cycle.

\section{Software Architecture}
The open-source implementation is organised to support experimentation, reporting, and deployment:
\begin{itemize}
  \item \textbf{Configurations}: YAML files in \texttt{configs/} encode dataset paths, episode budgets, exploration hyperparameters, and safety bands.
  \item \textbf{Training scripts}: \texttt{scripts/train\_*.py} instantiate the desired agent, set up logging, replay offline trajectories, and persist artefacts (plots, Excel scoreboards, JSON metrics) under \texttt{outputs/<timestamp>/}.
  \item \textbf{Utilities}: Shared helpers in \texttt{utils/} manage data ingestion, plotting, Excel exports, communication clients (TCP and Modbus), and optional MySQL persistence for live statistics.
  \item \textbf{Testing harness}: \texttt{scripts/test\_*.py} mirrors the training pipeline while streaming live weight measurements through the communication utilities, enabling shadow-mode trials before enabling closed-loop control.
  \item \textbf{Notebooks}: Exploratory notebooks and pre-generated histograms in \texttt{notebooks/} assist with diagnosing weight clusters and switch-point distributions.
\end{itemize}
The logging subsystem records every episode, including experienced and recommended switch points, exploration decisions, termination causes, and aggregated reward statistics. This transparency is essential for stakeholder sign-off before live deployment.

\section{Experimental Evaluation}
\subsection{Offline Replay Protocol}
Each experiment draws from the clustered historical sessions exposed by the \texttt{DataProcessor}. Episodes are sampled with replacement to ensure that every switch-point cluster remains available across thousands of iterations. Metrics written to \texttt{metrics.json} include the total number of episodes, the final greedy switch point, and convergence diagnostics extracted from the Q-value tables. Plots generated under each run visualise the evolution of action values and the exploration trajectory.

\subsection{Hyperparameter Selection}
Table~\ref{tab:hyperparams} summarises the default configuration. All agents share the same safety bounds and exploration weights. Penalty coefficients can be tuned to reflect production economics (e.g., scrap cost versus throughput impact).

\begin{table}[t]
  \caption{Default Hyperparameters Used in Offline Training}
  \label{tab:hyperparams}
  \centering
  \begin{tabular}{lccccccc}
    \toprule
    Algorithm & Episodes & $\alpha$ & $\gamma$ & $Q_0$ & $\epsilon_0$ & $\epsilon_{\min}$ & Decay \\
    \midrule
    MAB & 2000 & 0.40 & -- & 0.0 & 0.50 & 0.00 & 0.995 \\
    MC & 500 & 0.01 & 0.99 & -125 & 0.90 & 0.01 & 0.995 \\
    TD & 500 & 0.10 & 0.99 & -125 & 0.50 & 0.10 & 1.000 \\
    Q-Learning & 100 & 0.01 & 0.99 & 0.0 & 0.50 & 0.00 & 0.995 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Live Deployment Hooks}
The test scripts expose sockets compatible with the plant's TCP or Modbus endpoints. Incoming payloads are converted into floating-point weight readings, quantised, and appended to the current episode buffer. After every cycle, the system updates the database with raw and parsed traces plus optional Q-value snapshots, allowing remote dashboards to monitor safe, overflow, and underflow counts in near real time.

\section{Discussion}
Offline replay provided rapid feedback on convergence behaviour without disturbing production. The MAB controller reacts quickly to changing reward structures but may oscillate when penalties are small. MC and TD variants benefit from richer trajectory information, although they require careful handling of sparse state visitation. Q-learning delivered the most stable switching threshold and naturally supports policy extraction for supervisory control logic. The engineering stack---particularly the Excel exports and plots---proved essential for aligning with process engineers accustomed to spreadsheet-based validation.

\section{Conclusion}
We presented an IEEE Transactions-compliant manuscript describing the RL filling control project. The release combines data preparation, four complementary reinforcement learning agents, and industrial interfacing utilities. Future work includes incorporating model-based safety constraints, deploying on edge hardware for deterministic timing, and extending the action space to multi-stage filling strategies. The template offered here can be updated with live metrics and photographs once the pilot deployment concludes.

\section*{Acknowledgment}
The authors thank the production engineering team for granting access to historical filling logs and for their feedback on the dashboard requirements that informed the logging utilities.

\begin{thebibliography}{00}
\bibitem{Sutton2018} R. S. Sutton and A. G. Barto, \emph{Reinforcement Learning: An Introduction}, 2nd ed. Cambridge, MA, USA: MIT Press, 2018.
\bibitem{Lai1985} T. L. Lai and H. Robbins, "Asymptotically efficient adaptive allocation rules," \emph{Advances in Applied Mathematics}, vol. 6, no. 1, pp. 4--22, 1985.
\bibitem{Garcia2015} J. Garcia and F. Fernandez, "A comprehensive survey on safe reinforcement learning," \emph{Journal of Machine Learning Research}, vol. 16, pp. 1437--1480, May 2015.
\bibitem{Kober2013} J. Kober, J. A. Bagnell, and J. Peters, "Reinforcement learning in robotics: A survey," \emph{The International Journal of Robotics Research}, vol. 32, no. 11, pp. 1238--1274, Sept. 2013.
\end{thebibliography}

\end{document}
